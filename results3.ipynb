{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from math import log\n",
    "import numpy as np\n",
    "from analysis.ngram import calculate_avg_ln\n",
    "from analysis.optimization import optimize_em, optimize_gd, GradientDescent, EM\n",
    "from analysis.plot_utils import plt, legend_opts, savefig\n",
    "from matplotlib.animation import FuncAnimation"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Combine models sequentially"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "train_prob_matrix = np.load('../data/train_prob_matrix.npy')\n",
    "dev1_prob_matrix = np.load('../data/dev1_prob_matrix.npy')\n",
    "dev2_prob_matrix = np.load('../data/dev2_prob_matrix.npy')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "dev_prob_matrix = np.vstack([dev1_prob_matrix, dev2_prob_matrix])\n",
    "dev_prob_matrix.shape"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "train_avg_lls = []\n",
    "dev1_avg_lls = []\n",
    "dev2_avg_lls = []\n",
    "dev_avg_lls = []\n",
    "labels = []\n",
    "label = ''\n",
    "\n",
    "ngram_combo = []\n",
    "for n in range(6):\n",
    "    ngram_combo.append(n)\n",
    "    train_avg_lls.append(calculate_avg_ln(train_prob_matrix[:, ngram_combo]))\n",
    "    dev1_avg_lls.append(calculate_avg_ln(dev1_prob_matrix[:, ngram_combo]))\n",
    "    dev2_avg_lls.append(calculate_avg_ln(dev2_prob_matrix[:, ngram_combo]))\n",
    "    dev_avg_lls.append(calculate_avg_ln(dev_prob_matrix[:, ngram_combo]))\n",
    "    \n",
    "    label += f'{n}\\n'\n",
    "    labels.append(label)\n",
    "labels[0] = '0\\n(uniform)'"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(5, 5))\n",
    "\n",
    "ax.plot(ngram_combo, train_avg_lls, '-o', color='tab:red', clip_on=False)\n",
    "ax.plot(ngram_combo, dev1_avg_lls, '-o', color='tab:blue', clip_on=False)\n",
    "ax.plot(ngram_combo, dev2_avg_lls, '-o', color='tab:green', clip_on=False)\n",
    "ax.plot(ngram_combo, dev_avg_lls, '-o', color='tab:gray', clip_on=False)\n",
    "\n",
    "ax.text(5.1, train_avg_lls[-1]+0.1, s='train', va='center', color='tab:red', fontsize=15)\n",
    "ax.text(5.1, dev1_avg_lls[-1]+0.1, s='dev1', va='center', color='tab:blue', fontsize=15)\n",
    "ax.text(5.1, dev2_avg_lls[-1]+0.1, s='dev2', va='center', color='tab:green', fontsize=15)\n",
    "ax.text(5.1, dev_avg_lls[-1]+0.1, s='dev', va='center', color='tab:gray', fontsize=15)\n",
    "\n",
    "ax.set_xticklabels(labels)\n",
    "ax.set_xlabel('Model combinations\\n(equal weights)')\n",
    "ax.set_ylabel('Avearge log likelihood')\n",
    "ax.set_xlim(0, 5)\n",
    "ax.set_ylim(-15, 0)\n",
    "ax.set_yticks(range(-15, 1, 3))\n",
    "plt.show()\n",
    "\n",
    "savefig(fig, 'what_to_combine.png')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Interpolating 2 models"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "unigram_weights = np.linspace(0.0001, 1-0.0001, 100)\n",
    "dev1_avg_lls = []\n",
    "dev2_avg_lls = []\n",
    "dev_avg_lls = []\n",
    "\n",
    "for unigram_weight in unigram_weights:\n",
    "    dev1_avg_lls.append(calculate_avg_ln(dev1_prob_matrix[:, [0, 1]], weights=[1-unigram_weight, unigram_weight]))\n",
    "    dev2_avg_lls.append(calculate_avg_ln(dev2_prob_matrix[:, [0, 1]], weights=[1-unigram_weight, unigram_weight]))\n",
    "    dev_avg_lls.append(calculate_avg_ln(dev_prob_matrix[:, [0, 1]], weights=[1-unigram_weight, unigram_weight]))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(5, 5))\n",
    "\n",
    "# ax.plot(unigram_weights, dev1_avg_lls, color='tab:blue', clip_on=False)\n",
    "# ax.plot(unigram_weights, dev2_avg_lls, color='tab:green', clip_on=False)\n",
    "ax.plot(unigram_weights, dev_avg_lls, color='tab:gray', clip_on=False)\n",
    "\n",
    "# ax.scatter(unigram_weights[np.argmax(dev1_avg_lls)], np.max(dev1_avg_lls), color='tab:blue', clip_on=False)\n",
    "# ax.scatter(unigram_weights[np.argmax(dev2_avg_lls)], np.max(dev2_avg_lls), color='tab:green', clip_on=False)\n",
    "ax.scatter(unigram_weights[np.argmax(dev_avg_lls)], np.max(dev_avg_lls), color='tab:gray', clip_on=False, label='Max average log likelihood')\n",
    "\n",
    "ax.set_xlim(0, 1)\n",
    "ax.set_ylim(-10, -5)\n",
    "ax.legend(**legend_opts)\n",
    "ax.set_xlabel('Unigram weight')\n",
    "ax.set_ylabel('Avearge log likelihood')\n",
    "\n",
    "savefig(fig, 'avg_ll_two_models.png')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Gradient descent"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Animate gradient descent progress"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Run gradient descent for first 10 iterations (technically 11)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "optimize_gd(dev_prob_matrix[:, [0, 1]],\n",
    "            init_weights=[0.1, 0.9],\n",
    "            learning_rate=0.1,\n",
    "            n_iter=10)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "n_iter = 11\n",
    "iter_colors = plt.cm.viridis_r(np.linspace(0.2, 1, n_iter))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "gd = GradientDescent()\n",
    "gd.fit(dev_prob_matrix[:, [0, 1]], learning_rate=0.1, n_iter=n_iter, init_weights=[0.9, 0.1])\n",
    "gd.weights"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def update_gd(fig, ax, iteration):\n",
    "    fig.suptitle(f'Iteration {iteration}', fontsize=20)\n",
    "    ax.lines = [ax.lines[0]]\n",
    "    ax.collections = [ax.collections[0]]\n",
    "        \n",
    "    tangent_length = 0.15\n",
    "    \n",
    "    info = gd.tracked_info[iteration]\n",
    "    unigram_weight = info['weights'][1]\n",
    "    unigram_gradient = info['gradients'][0]\n",
    "    avg_ll = info['avg_ll']\n",
    "    \n",
    "    lower_unigram_weight, upper_unigram_weight = unigram_weight - tangent_length, unigram_weight + tangent_length\n",
    "    lower_ll, upper_ll = avg_ll - unigram_gradient * tangent_length, avg_ll + unigram_gradient * tangent_length\n",
    "    \n",
    "    # Plot current point\n",
    "    ax.scatter(unigram_weight, avg_ll, color=iter_colors[iteration])\n",
    "    \n",
    "    # Plot tangent line to objective function\n",
    "    ax.plot([lower_unigram_weight, upper_unigram_weight], [lower_ll, upper_ll], color=iter_colors[iteration])\n",
    "    \n",
    "    # Plot trace line up to current point\n",
    "    ax.vlines(unigram_weight, -10, avg_ll, colors=iter_colors[iteration], linestyles='dashed')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(7, 7))\n",
    "ax.plot(unigram_weights, dev_avg_lls, color='tab:gray', clip_on=False)\n",
    "ax.scatter(unigram_weights[np.argmax(dev_avg_lls)], np.max(dev_avg_lls), color='tab:gray', clip_on=False, label='Max average log likelihood')\n",
    "ax.set_xlim(0, 1)\n",
    "ax.set_ylim(-10, -5)\n",
    "\n",
    "ax.legend(**legend_opts)\n",
    "ax.set_xlabel('Unigram weight')\n",
    "ax.set_ylabel('Avearge log likelihood')\n",
    "\n",
    "anim = FuncAnimation(fig, lambda iteration: update_gd(fig, ax, iteration), \n",
    "                     frames=range(n_iter), interval=300, repeat=True)\n",
    "anim.save('../viz/gradient_descent.mp4', writer='ffmpeg')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Expectation-maximation algorithm"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### High-level explanation"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Two-model example (uniform + unigram)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def calculate_posterior_probs(prob_matrix, weights):\n",
    "    \"\"\"Calculate posterior weights for a unigram weight at the current iteration\"\"\"\n",
    "    weighted_probs = prob_matrix * weights\n",
    "    total_probs = weighted_probs.sum(axis=1, keepdims=True)\n",
    "    posterior_probs = weighted_probs / total_probs\n",
    "    return posterior_probs"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def calculate_lower_bound(prob_matrix, unigram_weight, unigram_step_weight):\n",
    "    \"\"\"Calculate lower bound value to each unigram weight\n",
    "    (parametized by posterior weights corresponding to unigram weight at current iteration).\n",
    "    This is used to plot the lower bound by inputting an array of unigram weights.\"\"\"\n",
    "    true_weights = [1 - unigram_weight, unigram_weight]\n",
    "    step_weights = [1 - unigram_step_weight, unigram_step_weight]\n",
    "    posterior_probs = calculate_posterior_probs(prob_matrix, step_weights)\n",
    "    with np.errstate(invalid='ignore'):\n",
    "        lower_bound_per_model = posterior_probs * np.log(prob_matrix * true_weights / posterior_probs)\n",
    "        # Convert any invalid log probabilities (such as prob = 0 -> log prob = -inf) into 0\n",
    "        # This won't affect the lower bound, since any models whose prob = 0\n",
    "        # should not be included in the lower bound in the first place\n",
    "        lower_bound_per_model[np.isnan(lower_bound_per_model)] = 0\n",
    "        lower_bound_ll = np.sum(lower_bound_per_model, axis=1)\n",
    "        lower_bound_avg_ll = lower_bound_ll.mean()\n",
    "    return lower_bound_avg_ll"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def construct_lower_bound(prob_matrix, unigram_weights, unigram_step_weight):\n",
    "    \"\"\"Construct lower bound for an array of unigram weights, and tight at specified unigram weight at a given step\"\"\"\n",
    "    return [calculate_lower_bound(prob_matrix, unigram_weight, unigram_step_weight) for unigram_weight in unigram_weights]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Step 1: Initialize starting weights"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "dev_lower_bound_avg_lls = []\n",
    "\n",
    "unigram_weight0 = 0.1\n",
    "avg_ll0 = calculate_avg_ln(dev_prob_matrix[:, [0, 1]], weights=[1-unigram_weight0, unigram_weight0])\n",
    "\n",
    "unigram_weights = np.linspace(0.0001, 1-0.0001, 100)\n",
    "objective_function = [calculate_avg_ln(dev_prob_matrix[:, [0, 1]], weights=[1-unigram_weight, unigram_weight]) for unigram_weight in unigram_weights]\n",
    "argmax_objective_function, max_objective_function = unigram_weights[np.argmax(objective_function)], np.max(objective_function)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(5, 5))\n",
    "\n",
    "# Plot objective function\n",
    "ax.plot(unigram_weights, objective_function, color='tab:gray', label='Objective function')\n",
    "ax.scatter(argmax_objective_function, max_objective_function, color='tab:gray')\n",
    "\n",
    "# Plot starting weights\n",
    "ax.scatter(unigram_weight0, avg_ll0, color=iter_colors[0], label='Starting weights', zorder=10)\n",
    "ax.vlines(unigram_weight0, -10, avg_ll0, color=iter_colors[0], linestyles='dashed')\n",
    "ax.legend(**legend_opts)\n",
    "\n",
    "ax.set_xlim(0, 1)\n",
    "ax.set_ylim(-10, -5)\n",
    "ax.set_xlabel('Unigram weight')\n",
    "ax.set_ylabel('Avearge log likelihood')\n",
    "savefig(fig, 'e1.png')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Step 2: Construct lower bound to objective function that is tight at starting weights (E-step)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "lower_bounds0 = construct_lower_bound(dev_prob_matrix[:, [0, 1]], unigram_weights, unigram_weight0)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(5, 5))\n",
    "\n",
    "# Plot objective function\n",
    "ax.plot(unigram_weights, objective_function, color='tab:gray', label='Objective function')\n",
    "ax.scatter(argmax_objective_function, max_objective_function, color='tab:gray')\n",
    "\n",
    "# Plot starting weights\n",
    "ax.scatter(unigram_weight0, avg_ll0, color=iter_colors[0], label='Starting weights', zorder=10)\n",
    "ax.vlines(unigram_weight0, -10, avg_ll0, color=iter_colors[0], linestyles='dashed')\n",
    "\n",
    "# Plot lower bound\n",
    "ax.plot(unigram_weights, lower_bounds0, color=iter_colors[0], label='Lower bound')\n",
    "ax.legend(**legend_opts)\n",
    "\n",
    "ax.set_xlim(0, 1)\n",
    "ax.set_ylim(-10, -5)\n",
    "ax.set_xlabel('Unigram weight')\n",
    "savefig(fig, 'e2.png')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "**Step 3**: Find weights that maximize lower bound from step 2 and update weights to these values"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "unigram_weight1, lower_bound1 = unigram_weights[np.argmax(lower_bounds0)], np.max(lower_bounds0)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(5, 5))\n",
    "\n",
    "# Plot objective function\n",
    "ax.plot(unigram_weights, objective_function, color='tab:gray', label='Objective function')\n",
    "ax.scatter(argmax_objective_function, max_objective_function, color='tab:gray')\n",
    "\n",
    "# Plot starting weights\n",
    "ax.scatter(unigram_weight0, avg_ll0, color=iter_colors[0], label='Starting weights', zorder=10)\n",
    "ax.vlines(unigram_weight0, -10, avg_ll0, iter_colors[0], linestyles='dashed')\n",
    "\n",
    "# Plot lower bound\n",
    "ax.plot(unigram_weights, lower_bounds0, color=iter_colors[0], label='Lower bound')\n",
    "\n",
    "# Plot max lower bound\n",
    "ax.scatter(unigram_weight1, lower_bound1, color=iter_colors[1], label='Max lower bound', zorder=10)\n",
    "ax.vlines(unigram_weight1, -10, lower_bound1, color=iter_colors[1], linestyles='dashed')\n",
    "\n",
    "ax.legend(**legend_opts)\n",
    "\n",
    "ax.set_xlim(0, 1)\n",
    "ax.set_ylim(-10, -5)\n",
    "ax.set_xlabel('Unigram weight')\n",
    "savefig(fig, 'e3.png')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "**Step 4**: Repeat step 2 and 3"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "lower_bounds1 = construct_lower_bound(dev_prob_matrix[:, [0, 1]], unigram_weights, unigram_weight1)\n",
    "avg_ll1 = calculate_avg_ln(dev_prob_matrix[:, [0, 1]], weights=[1-unigram_weight1, unigram_weight1])\n",
    "unigram_weight2, lower_bound2 = unigram_weights[np.argmax(lower_bounds1)], np.max(lower_bounds1)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(5, 5))\n",
    "\n",
    "# Plot objective function\n",
    "ax.plot(unigram_weights, objective_function, color='tab:gray', label='Objective function')\n",
    "ax.scatter(argmax_objective_function, max_objective_function, color='tab:gray')\n",
    "\n",
    "# Plot updated weights\n",
    "ax.scatter(unigram_weight1, avg_ll1, color=iter_colors[1], label='Updated weights', zorder=10)\n",
    "ax.vlines(unigram_weight1, -10, avg_ll1, color=iter_colors[1], linestyles='dashed')\n",
    "\n",
    "# Plot lower bound\n",
    "ax.plot(unigram_weights, lower_bounds1, color=iter_colors[1], label='Lower bound')\n",
    "\n",
    "# Plot (new) max lower bound\n",
    "ax.scatter(unigram_weight2, lower_bound2, color=iter_colors[2], label='Max lower bound', zorder=10)\n",
    "ax.vlines(unigram_weight2, -10, lower_bound2, color=iter_colors[2], linestyles='dashed')\n",
    "\n",
    "ax.legend(**legend_opts)\n",
    "\n",
    "ax.set_xlim(0, 1)\n",
    "ax.set_ylim(-10, -5)\n",
    "ax.set_xlabel('Unigram weight')\n",
    "savefig(fig, 'e4.png')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Why does the lower bound need to be tight?"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Tight lower bound"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(5, 5))\n",
    "\n",
    "# Plot objective function\n",
    "ax.plot(unigram_weights, objective_function, color='tab:gray', label='Objective function')\n",
    "ax.scatter(argmax_objective_function, max_objective_function, color='tab:gray')\n",
    "\n",
    "# Plot starting weight\n",
    "ax.scatter(unigram_weight0, avg_ll0, color=iter_colors[0], label='Starting weights', zorder=10)\n",
    "ax.vlines(unigram_weight0, -10, avg_ll0, iter_colors[0], linestyles='dashed')\n",
    "\n",
    "# Plot lower bound\n",
    "ax.plot(unigram_weights, lower_bounds0, color=iter_colors[0], label='Lower bound')\n",
    "\n",
    "# Plot max lower bound\n",
    "ax.scatter(unigram_weight1, lower_bound1, color=iter_colors[1], label='Max lower bound', zorder=10)\n",
    "ax.vlines(unigram_weight1, -10, avg_ll1, color=iter_colors[1], linestyles='dashed')\n",
    "\n",
    "# Plot average log likelihood of max lower bound\n",
    "ax.scatter(unigram_weight1, avg_ll1, color=iter_colors[1], zorder=10)\n",
    "\n",
    "ax.legend(**legend_opts, loc='upper right')\n",
    "\n",
    "ax.set_xlim(0, 1)\n",
    "ax.set_ylim(-10, -5)\n",
    "ax.set_xlabel('Unigram weight')\n",
    "ax.set_ylabel('Average log likelihood')\n",
    "savefig(fig, 'tight_lower_bound.png')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Lose lower bound"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "unigram_weight_loose = 0.9999\n",
    "\n",
    "# Create tight lower bound at 0.9, then flip it horizontally and shift down to create loose lowerbound\n",
    "lower_bounds_loose0 = construct_lower_bound(dev_prob_matrix[:, [0, 1]], unigram_weights, unigram_weight_loose)\n",
    "lower_bounds_loose0 = np.array(lower_bounds_loose0[::-1]) - 2\n",
    "\n",
    "# Record lower bound value at starting weight\n",
    "lower_bound_loose0 = calculate_lower_bound(dev_prob_matrix[:, [0, 1]], 1-unigram_weight0, unigram_weight_loose) - 2\n",
    "\n",
    "# Record weights that maximizes lower bound\n",
    "unigram_weight_loose1, lower_bound_loose1 = unigram_weights[np.argmax(lower_bounds_loose0)], np.max(lower_bounds_loose0)\n",
    "\n",
    "# Record objective function at lower bound-maximizing weights\n",
    "avg_ll_loose1 = calculate_avg_ln(dev_prob_matrix[:, [0, 1]], weights=[1-unigram_weight_loose1, unigram_weight_loose1])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(5, 5))\n",
    "\n",
    "# Plot objective function\n",
    "ax.plot(unigram_weights, objective_function, color='tab:gray', label='Objective function')\n",
    "ax.scatter(argmax_objective_function, max_objective_function, color='tab:gray')\n",
    "\n",
    "# Plot starting weights\n",
    "ax.scatter(unigram_weight0, avg_ll0, color=iter_colors[0], label='Starting weights', zorder=10)\n",
    "ax.vlines(unigram_weight0, -10, avg_ll0, color=iter_colors[0], linestyles='dashed')\n",
    "\n",
    "# Plot loose lower bound\n",
    "ax.plot(unigram_weights, lower_bounds_loose0, color=iter_colors[0], label='Lower bound')\n",
    "\n",
    "# Plot average log likelihood of starting weight at loose lower bound\n",
    "ax.scatter(unigram_weight0, lower_bound_loose0, color=iter_colors[0])\n",
    "\n",
    "# Plot max lower bound\n",
    "ax.scatter(unigram_weight_loose1, lower_bound_loose1, color=iter_colors[1], label='Max lower bound', zorder=10)\n",
    "ax.vlines(unigram_weight_loose1, -10, avg_ll_loose1, color=iter_colors[1], linestyles='dashed')\n",
    "\n",
    "# Plot average log likelihood of max lower bound\n",
    "ax.scatter(unigram_weight_loose1, avg_ll_loose1, color=iter_colors[1], zorder=10)\n",
    "\n",
    "ax.legend(**legend_opts, loc='upper right')\n",
    "\n",
    "ax.set_xlim(0, 1)\n",
    "ax.set_ylim(-10, -5)\n",
    "ax.set_xlabel('Unigram weight')\n",
    "ax.set_ylabel('Average log likelihood')\n",
    "savefig(fig, 'loose_lower_bound.png')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Jensen's inequality"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "xs = np.linspace(0.1, 1, 1000)\n",
    "ys = np.log(xs)\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(5, 5))\n",
    "ax.axhline(0, color='black', lw=1)\n",
    "ax.set_xlim(0, 1)\n",
    "ax.set_ylim(log(0.1), 0)\n",
    "\n",
    "def get_secant_value(x, x_start, x_end):\n",
    "    return log(x_start) + (log(x_end) - log(x_start)) * (x - x_start) / (x_end - x_start)\n",
    "# Log\n",
    "ax.plot(xs, ys, zorder=11, color='tab:brown')\n",
    "\n",
    "# Secant line\n",
    "ax.plot([0.2, 0.8], [log(0.2), log(0.8)], color='tab:gray', lw=1, zorder=11)\n",
    "\n",
    "# \n",
    "ax.scatter(0.4, log(0.4), zorder=12, color='tab:brown')\n",
    "secant_at_04 = get_secant_value(0.4, x_start=0.2, x_end=0.8)\n",
    "ax.scatter(0.4, secant_at_04, color='tab:gray', zorder=12)\n",
    "ax.vlines(0.2, log(0.2), 0, color='tab:gray', lw=1, ls='--', zorder=10)\n",
    "ax.vlines(0.8, log(0.8), 0, color='tab:gray', lw=1, ls='--', zorder=10)\n",
    "ax.vlines(0.4, secant_at_04, 0, color='black', lw=1, ls='--', zorder=10)\n",
    "\n",
    "savefig(fig, 'log_jensen.png')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Lower bound as expectation"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "optimize_gd(prob_matrix=dev_prob_matrix[:, [0, 1]],\n",
    "            learning_rate=0.1,\n",
    "            n_iter=10,\n",
    "            init_weights=[0.1, 0.9])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(5, 5))\n",
    "\n",
    "ax.bar([0, 1], [0.32, 0.68], color='tab:gray')\n",
    "ax.set_xticks([])\n",
    "ax.set_ylim(0, 1)\n",
    "\n",
    "ax.axvline(0, -0.02, 0, clip_on=False, color='tab:gray')\n",
    "ax.axvline(1, -0.02, 0, clip_on=False, color='tab:gray')\n",
    "\n",
    "savefig(fig, 'z_dist.png')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Code EM algorithm"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "optimize_em(prob_matrix=dev_prob_matrix[:, [0, 1]],\n",
    "            n_iter=10,\n",
    "            init_weights=[0.9, 0.1])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "n_iter = 11\n",
    "em = EM()\n",
    "em.fit(dev_prob_matrix[:, [0, 1]], init_weights=[0.9, 0.1], n_iter=n_iter)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "unigram_weights = np.linspace(0.0001, 1-0.0001, 100)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def update_em(fig, ax, iteration):\n",
    "    print(iteration)\n",
    "    fig.suptitle(f'Iteration {iteration}', fontsize=20)\n",
    "    ax.collections = [ax.collections[0]]\n",
    "    ax.lines = [ax.lines[0]]\n",
    "    \n",
    "    unigram_step_weight = em.tracked_info[iteration]['weights'][1]\n",
    "    avg_ll = calculate_avg_ln(dev_prob_matrix[:, [0, 1]], [1 - unigram_step_weight, unigram_step_weight])\n",
    "    \n",
    "    # Plot current point\n",
    "    ax.scatter(unigram_step_weight, avg_ll, color=iter_colors[iteration])\n",
    "    \n",
    "    # Plot lower bound that is tight against objective function at current point\n",
    "    lower_bounds = construct_lower_bound(dev_prob_matrix[:, [0, 1]], unigram_weights, unigram_step_weight)\n",
    "    ax.plot(unigram_weights, lower_bounds, color=iter_colors[iteration])\n",
    "    \n",
    "    # Plot trace line up to current point\n",
    "    ax.vlines(unigram_step_weight, -10, avg_ll, color=iter_colors[iteration], linestyles='dashed')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(7, 7))\n",
    "ax.plot(unigram_weights, dev_avg_lls, color='tab:gray', clip_on=False)\n",
    "ax.scatter(unigram_weights[np.argmax(dev_avg_lls)], np.max(dev_avg_lls), color='tab:gray', clip_on=False, label='Max average log likelihood')\n",
    "ax.set_xlim(0, 1)\n",
    "ax.set_ylim(-10, -5)\n",
    "\n",
    "ax.legend(**legend_opts)\n",
    "ax.set_xlabel('Unigram weight')\n",
    "ax.set_ylabel('Avearge log likelihood')\n",
    "\n",
    "anim = FuncAnimation(fig, lambda iteration: update_em(fig, ax, iteration), \n",
    "                     frames=range(n_iter), interval=300, repeat=True)\n",
    "anim.save('../viz/em.mp4', writer='ffmpeg')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Compare EM with gradient descent"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def update_em_and_gd(fig, ax1, ax2, iteration):\n",
    "    update_gd(fig, ax1, iteration)\n",
    "    update_em(fig, ax2, iteration)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 6))\n",
    "ax1.plot(unigram_weights, dev_avg_lls, color='tab:gray', clip_on=False)\n",
    "ax1.scatter(unigram_weights[np.argmax(dev_avg_lls)], np.max(dev_avg_lls), color='tab:gray', clip_on=False, label='Max average log likelihood')\n",
    "ax1.set_xlim(0, 1)\n",
    "ax1.set_ylim(-10, -5)\n",
    "ax1.set_ylabel('Avearge log likelihood')\n",
    "ax1.set_xlabel('Unigram weight')\n",
    "ax1.set_title('Gradient descent')\n",
    "\n",
    "ax2.plot(unigram_weights, dev_avg_lls, color='tab:gray', clip_on=False)\n",
    "ax2.scatter(unigram_weights[np.argmax(dev_avg_lls)], np.max(dev_avg_lls), color='tab:gray', clip_on=False, label='Max average log likelihood')\n",
    "ax2.set_xlim(0, 1)\n",
    "ax2.set_ylim(-10, -5)\n",
    "\n",
    "ax2.legend(**legend_opts)\n",
    "ax2.set_xlabel('Unigram weight')\n",
    "ax2.set_title('Expectation-maximization')\n",
    "\n",
    "anim = FuncAnimation(fig, lambda iteration: update_em_and_gd(fig, ax1, ax2, iteration), \n",
    "                     frames=range(n_iter), interval=300, repeat=True)\n",
    "anim.save('../viz/em_vs_gd.mp4', writer='ffmpeg')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Plot EM vs gradient descent at various learning rates"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Uniform-unigram interpolation"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(5, 5))\n",
    "\n",
    "n_iter = 51\n",
    "learning_rates = [0.1, 0.2, 0.3, 0.4]\n",
    "rate_colors = ['tab:blue', 'tab:green', 'tab:orange', 'tab:red']\n",
    "iterations = list(range(n_iter))\n",
    "\n",
    "# Plot average log likelihood during EM\n",
    "em = EM()\n",
    "em.fit(dev_prob_matrix[:, [0, 1]], n_iter=n_iter)\n",
    "em_avg_lls = [em.tracked_info[iteration]['avg_ll'] for iteration in iterations]\n",
    "ax.plot(iterations, em_avg_lls, color='black', lw=3, label='EM', clip_on=False)\n",
    "\n",
    "# Plot average log likelihood during gradient descent (different learning rates)\n",
    "for learning_rate, color in zip(learning_rates, rate_colors):\n",
    "    gd = GradientDescent()\n",
    "    gd.fit(dev_prob_matrix[:, [0, 1]], learning_rate=learning_rate, n_iter=n_iter)\n",
    "    gd_avg_lls = [gd.tracked_info[iteration]['avg_ll'] for iteration in iterations]\n",
    "    ax.plot(iterations, gd_avg_lls, color=color, label=f'GD (learning rate = {learning_rate})', clip_on=False)\n",
    "    \n",
    "ax.legend(**legend_opts, loc='lower right')\n",
    "ax.set_xlabel('Iteration')\n",
    "ax.set_ylabel('Average log likelihood')\n",
    "ax.set_ylim(-7, -6.75)\n",
    "ax.set_xlim(0, 50)\n",
    "plt.show()\n",
    "\n",
    "savefig(fig, 'em_gd_convergence_twomodel.png')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "All model interpolation"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(5, 5))\n",
    "\n",
    "n_iter = 51\n",
    "learning_rates = [0.1, 0.01, 0.001, 0.0001]\n",
    "rate_colors = ['tab:red', 'tab:orange', 'tab:green', 'tab:blue']\n",
    "iterations = list(range(n_iter))\n",
    "\n",
    "# Plot average log likelihood during EM\n",
    "em = EM()\n",
    "em.fit(dev_prob_matrix, n_iter=n_iter)\n",
    "em_avg_lls = [em.tracked_info[iteration]['avg_ll'] for iteration in iterations]\n",
    "ax.plot(iterations, em_avg_lls, color='black', lw=3, label='EM', clip_on=False)\n",
    "\n",
    "# Plot average log likelihood during gradient descent (different learning rates)\n",
    "for learning_rate, rate_color in zip(learning_rates, rate_colors):\n",
    "    gd = GradientDescent()\n",
    "    gd.fit(dev_prob_matrix, learning_rate=learning_rate, n_iter=n_iter)\n",
    "    gd_avg_lls = [gd.tracked_info[iteration]['avg_ll'] for iteration in iterations]\n",
    "    ax.plot(iterations, gd_avg_lls, color=rate_color, label=f'GD (learning rate = {learning_rate})')\n",
    "    \n",
    "ax.legend(bbox_to_anchor=(1.04,0), loc='lower left', **legend_opts)\n",
    "\n",
    "ax.set_xlabel('Iteration')\n",
    "ax.set_ylabel('Average log likelihood')\n",
    "ax.set_ylim(-6.6, -6.1)\n",
    "ax.set_xlim(0, 50)\n",
    "plt.show()\n",
    "\n",
    "savefig(fig, 'em_gd_convergence_full.png')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Result of EM"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "n_iter = 11\n",
    "em = EM()\n",
    "em.fit(dev_prob_matrix, n_iter=n_iter)\n",
    "\n",
    "iterations = list(range(n_iter))\n",
    "em_avg_lls = [em.tracked_info[iteration]['avg_ll'] for iteration in iterations]\n",
    "em_weight_percents = {}\n",
    "\n",
    "ngram_lengths = list(range(6))\n",
    "ngram_colors = ['tab:gray', 'tab:blue', 'tab:green', 'tab:orange', 'tab:brown', 'tab:red']\n",
    "ngram_labels = ['Uniform', 'Unigram', 'Bigram', 'Trigram', '4-gram', '5-gram']\n",
    "for ngram_length in ngram_lengths:\n",
    "    em_weight_percents[ngram_length] = [em.tracked_info[iteration]['weights'][ngram_length]*100 for iteration in iterations]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "fig, (ax1, ax2) = plt.subplots(2, 1, figsize=(5, 10))\n",
    "ax1.set_xticks(range(11))\n",
    "ax1.set_xlim(0, 10)\n",
    "ax1.set_ylim(-6.6, -6.1)\n",
    "ax1.set_ylabel('Average log likelihood')\n",
    "\n",
    "\n",
    "ax2.set_xlim(0, 10)\n",
    "ax2.set_ylim(0, 50)\n",
    "ax2.set_xticks(range(11))\n",
    "ax2.set_xlabel('Iteration')\n",
    "ax2.set_ylabel('Interpolation weight (%)')\n",
    "\n",
    "\n",
    "for ngram_length, ngram_color, ngram_label in zip(ngram_lengths, ngram_colors, ngram_labels):\n",
    "    ax2.plot(iterations, em_weight_percents[ngram_length], color=ngram_color, marker='o', clip_on=False, label=ngram_label)\n",
    "    \n",
    "ax1.plot(iterations, em_avg_lls, color='black', marker='o', clip_on=False)\n",
    "\n",
    "ax2.legend(**legend_opts, bbox_to_anchor=(1.04,0), loc='lower left')\n",
    "\n",
    "savefig(fig, 'em_result.png')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "em.tracked_info[0]['avg_ll'], em.tracked_info[10]['avg_ll']"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "em.tracked_info[n_iter-1]['weights'] * 100"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Compare EM result with average interpolation"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "n_iter = 51\n",
    "em = EM()\n",
    "em.fit(dev_prob_matrix, n_iter=n_iter)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "test1_prob_matrix = np.load('../data/test1_prob_matrix.npy')\n",
    "test2_prob_matrix = np.load('../data/test2_prob_matrix.npy')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "dev1_naive, dev1_em = calculate_avg_ln(dev1_prob_matrix), calculate_avg_ln(dev1_prob_matrix, em.weights)\n",
    "dev2_naive, dev2_em = calculate_avg_ln(dev2_prob_matrix), calculate_avg_ln(dev2_prob_matrix, em.weights)\n",
    "test1_naive, test1_em = calculate_avg_ln(test1_prob_matrix), calculate_avg_ln(test1_prob_matrix, em.weights)\n",
    "test2_naive, test2_em = calculate_avg_ln(test2_prob_matrix), calculate_avg_ln(test2_prob_matrix, em.weights)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(5, 5))\n",
    "ax.set_ylim(-7, -5.6)\n",
    "ax.set_xticks([0, 1])\n",
    "ax.set_xticklabels(['Equal\\nweights', 'EM-optimized\\nweights'])\n",
    "ax.set_ylabel('Average log likelihood')\n",
    "\n",
    "ax.plot([0, 1], [dev1_naive, dev1_em], color='tab:blue', marker='o')\n",
    "ax.plot([0, 1], [dev2_naive, dev2_em], color='tab:green', marker='o')\n",
    "ax.plot([0, 1], [test1_naive, test1_em], color='tab:cyan', marker='o')\n",
    "ax.plot([0, 1], [test2_naive, test2_em], color='tab:olive', marker='o')\n",
    "\n",
    "savefig(fig, 'em_vs_equal.png')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}